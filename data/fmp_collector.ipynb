{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b10ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "PAGE_NUM = 0\n",
    "ARTICLES = 20 # max 20\n",
    "API_KEY = \"pwm4EP6mJg6p3Ys2STFagWEkQMD0NqD3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52be08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read both CSV files\n",
    "df1 = pd.read_csv('fmp_1.csv')\n",
    "df2 = pd.read_csv('fmp_2.csv')\n",
    "\n",
    "# Stack them vertically\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save to new CSV file\n",
    "combined_df.to_csv('fmp_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c76250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_text(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Заменяем <p> на переносы строк и удаляем пустые строки\n",
    "    for p in soup.find_all('p'):\n",
    "        p.append('\\n\\n')\n",
    "    \n",
    "    # Получаем текст и обрабатываем\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Удаляем лишние переносы и пробелы\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    clean_text = '\\n'.join(lines)\n",
    "    \n",
    "    return clean_text.strip()\n",
    "\n",
    "\n",
    "def parse_fmp_articles(data):\n",
    "    articles = []\n",
    "    for article in data[\"content\"]:\n",
    "        # Очистка HTML-тегов из контента\n",
    "        clean_content = clean_html_text(article[\"content\"])\n",
    "        \n",
    "        articles.append({\n",
    "            \"title\": article[\"title\"],\n",
    "            \"date\": article[\"date\"],\n",
    "            \"content\": clean_content,  # Текст без HTML\n",
    "            \"tickers\": article[\"tickers\"],\n",
    "            \"image\": article[\"image\"],\n",
    "            \"link\": article[\"link\"],\n",
    "            \"author\": article[\"author\"],\n",
    "            \"source\": article[\"site\"]\n",
    "        })\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e462539",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "\n",
    "for page in range(52, 102):  # 20 articles per page → 50 pages = 1,000 articles\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/fmp/articles?page={page}&size={ARTICLES}&apikey={API_KEY}\"\n",
    "    # https://financialmodelingprep.com/api/v3/fmp/articles?page=2&size=20&apikey=pwm4EP6mJg6p3Ys2STFagWEkQMD0NqD3\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    arts = parse_fmp_articles(response)\n",
    "\n",
    "    articles.extend(arts)\n",
    "\n",
    "df = pd.DataFrame(articles)[[\"content\"]]\n",
    "df.to_csv(\"fmp_2000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51996733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Arteris, Inc. (AIP) Wins AI Engineering Innovation Award', 'date': '2025-06-27 18:00:07', 'content': 'Arteris, Inc. (NASDAQ:AIP) received the \"AI Engineering Innovation Award\" for its FlexGen technology, enhancing system-on-chip (SoC) development for AI applications.\\nFlexGen technology significantly improves NoC design efficiency, with early adopters reporting a tenfold productivity increase and notable reductions in wire length and latency.\\nDespite its technological advancements, Arteris faces financial challenges, including a negative P/E ratio of -12.18 and a high enterprise value to operating cash flow ratio of 236.35, indicating potential overvaluation.\\nArteris, Inc. (Nasdaq: AIP) is a key player in the semiconductor industry, specializing in system IP for system-on-chip (SoC) development. The company recently received the \"AI Engineering Innovation Award\" at the 2025 AI Breakthrough Awards for its FlexGen technology. This innovation addresses the challenges of network-on-chip (NoC) design, particularly for AI-centric semiconductors used in data centers and self-driving cars.\\nFlexGen automates NoC creation, enhancing performance and efficiency for AI-driven SoCs. Early adopters report a tenfold productivity increase, a 30% reduction in wire length, and a 10% latency reduction. This technology also offers manual editing through FlexNoC 5, complementing its automated features. Despite these advancements, AIP\\'s financial metrics reveal challenges, such as a negative P/E ratio of -12.18, indicating current unprofitability.\\nThe AI Breakthrough Awards recognize innovation in AI, with over 5,000 nominations in 2025. Arteris\\' FlexGen technology plays a crucial role in advancing AI silicon designs. However, financial figures show a price-to-sales ratio of 6.60 and an enterprise value to sales ratio of 6.41, suggesting investors are paying a premium for each dollar of sales, despite the company\\'s negative earnings.\\nArteris\\' financial health is mixed. The company has a debt-to-equity ratio of -0.94, indicating more equity than debt, and a current ratio of 1.02, suggesting reasonable short-term financial health. However, the enterprise value to operating cash flow ratio is high at 236.35, pointing to potential overvaluation based on cash flow. Despite these challenges, Arteris remains committed to innovation in the AI landscape.', 'tickers': 'NASDAQ:AIP', 'image': 'https://portal.financialmodelingprep.com/positions/685ee655382870c8df57a9bc.png', 'link': 'https://financialmodelingprep.com/market-news/arteris-inc-wins-ai-engineering-innovation-award', 'author': 'Stuart Mooney', 'source': 'Financial Modeling Prep'}, {'title': 'EMCOR Group, Inc. (NYSE:EME) Sees Positive Analyst Coverage and Strong Market Performance', 'date': '2025-06-27 17:00:15', 'content': 'TD Securities initiated coverage on EMCOR Group, Inc. (NYSE:EME) with a \"Buy\" rating, indicating a strong market position and growth potential.\\nEMCOR\\'s stock performance has outpaced the broader market indices and the Construction sector, with a significant increase over the past month.\\nThe company is expected to report a robust earnings per share (EPS) and revenue growth in its upcoming earnings report, highlighting its financial strength.\\nEMCOR Group, Inc. (NYSE:EME) is a leading provider of mechanical and electrical construction services, industrial and energy infrastructure, and building services. The company operates across various sectors, including commercial, industrial, utility, and institutional markets. EMCOR competes with other major players in the construction and engineering industry, such as Fluor Corporation and Jacobs Engineering Group.\\nOn June 27, 2025, TD Securities initiated coverage on EMCOR Group with a \"Buy\" rating, as reported by Benzinga. At that time, the stock was priced at $528.85. This positive outlook from TD Securities highlights EMCOR\\'s strong market position and potential for growth, making it one of the top five initiations for that Friday.\\nRecently, EMCOR Group closed a trading session at $500.65, marking a 1.22% increase from its previous close. This performance surpassed the broader market indices, with the S&P 500 gaining 1.11%, the Dow rising by 1.19%, and the Nasdaq adding 1.43%. Over the past month, EMCOR\\'s shares have risen by 6.75%, significantly outperforming the Construction sector\\'s 2.35% gain and the S&P 500\\'s 3.92% increase.\\nInvestors are eagerly awaiting EMCOR Group\\'s upcoming earnings report. The company is expected to announce an earnings per share (EPS) of $5.68, reflecting an 8.19% increase from the same quarter last year. Additionally, revenue is projected to reach $4.1 billion, representing an 11.85% growth compared to the corresponding quarter of the previous year. These figures indicate EMCOR\\'s strong financial performance and growth potential.\\nCurrently, EME is priced at $530.74, showing an increase of 3.87% or $19.75. During the day, the stock fluctuated between a low of $517.65 and a high of $532.49. Over the past year, EME has reached a high of $545.29 and a low of $319.49. The company\\'s market capitalization is approximately $23.75 billion, with a trading volume of 193,492 shares on the NYSE.', 'tickers': 'NYSE:EME', 'image': 'https://portal.financialmodelingprep.com/positions/685ee5d2382870c8df57a7ad.jpeg', 'link': 'https://financialmodelingprep.com/market-news/emcor-group-inc-nyse-eme-positive-analyst-coverage-strong-market-performance', 'author': 'Andrew Wynn', 'source': 'Financial Modeling Prep'}, {'title': 'Nike Jumps 18% After Q4 Beat and Signs Turnaround Headwinds Are Easing', 'date': '2025-06-27 16:41:00', 'content': 'Shares of Nike (NYSE:NKE) surged over 18% intra-day today after the athletic giant reported fiscal fourth-quarter results that exceeded expectations and signaled that the worst financial impact from its turnaround plan is likely behind it.\\nNike posted Q4 sales of $11.10 billion, down 12% year-over-year but better than analysts anticipated, helped by signs of stabilization in its core running category. North American sales dropped 11% to $4.7 billion, a decline that was milder than feared.\\nEarnings per share were $0.14, topping the consensus estimate of $0.12. CEO Elliott Hill’s upbeat remarks during the earnings call—highlighting that the business is poised to improve from here—fueled investor optimism. “It’s time to turn the page,” Hill declared.\\nExecutives also outlined plans to shift more production from China to the U.S., aiming to mitigate potential cost increases from broad U.S. tariffs.\\nLooking ahead, Nike guided for first-quarter revenue to decline by a mid-single-digit percentage, a forecast more positive than analyst expectations for a 7.3% drop. Management noted that Q4 marked the peak of financial drag from its turnaround initiatives, and they anticipate these headwinds to ease going forward.', 'tickers': 'NYSE:NKE', 'image': 'https://cdn.financialmodelingprep.com/images/fmp-1751042460145.jpg', 'link': 'https://financialmodelingprep.com/market-news/fmp-nike-jumps-18-after-q4-beat-and-signs-turnaround-headwinds-are-easing', 'author': 'Davit Kirakosyan', 'source': 'Financial Modeling Prep'}, {'title': 'Concentrix Posts Q2 EPS Miss, But Revenue Beats & Strong Guidance Offers Rebound Potential', 'date': '2025-06-27 16:38:00', 'content': 'Concentrix Corporation (NASDAQ:CNXC) posted second-quarter earnings that came in below expectations, but revenue exceeded forecasts.\\nFor the quarter, the firm reported adjusted EPS of $2.70, missing the $2.78 consensus estimate. Revenue rose 1.5% year-over-year to $2.42 billion, slightly above the $2.38 billion forecast.\\nLooking ahead, Concentrix guided for third-quarter revenue of $2.445–$2.47 billion, comfortably above analyst estimates of $2.392 billion. The company expects Q3 adjusted EPS of $2.80–$2.91, bracketing the $2.90 consensus.\\nFor full-year fiscal 2025, Concentrix forecasts revenue of $9.72–$9.815 billion and adjusted EPS of $11.53–$11.76, both ahead of Street expectations, signaling confidence in accelerating momentum despite the Q2 earnings shortfall.', 'tickers': 'NASDAQ:CNXC', 'image': 'https://cdn.financialmodelingprep.com/images/fmp-1751042312790.jpg', 'link': 'https://financialmodelingprep.com/market-news/fmp-concentrix-posts-q2-eps-miss-but-revenue-beats-&-strong-guidance-offers-rebound-potential', 'author': 'Davit Kirakosyan', 'source': 'Financial Modeling Prep'}, {'title': 'Apogee Shares Surge 8% After Q1 Beat Despite Tariff Headwinds', 'date': '2025-06-27 16:35:00', 'content': 'Apogee Enterprises (NASDAQ:APOG) shares surged over 8% intra-day today after the architectural products manufacturer posted stronger-than-expected first-quarter results and raised its full-year guidance.\\nFor the quarter, Apogee reported adjusted earnings of $0.56 per share, beating analyst estimates of $0.49. Revenue rose 4.6% year-over-year to $346.6 million, also topping the $331.1 million consensus.\\nCEO Ty R. Silberhorn highlighted the company’s ability to deliver above-expectation results despite challenging market conditions and year-over-year headwinds.\\nLooking ahead, Apogee lifted its fiscal 2026 outlook, projecting net sales of $1.40–$1.44 billion, up from a prior range of $1.37–$1.43 billion. Adjusted EPS guidance was also raised to $3.80–$4.20, compared to $3.55–$4.10 previously, even as the company anticipates a $0.35–$0.45 per share impact from tariffs concentrated in the first half of the fiscal year.\\nThe strong quarter and improved forecast underscored resilience in Apogee’s business and reassured investors despite ongoing cost pressures.', 'tickers': 'NASDAQ:APOG', 'image': 'https://cdn.financialmodelingprep.com/images/fmp-1751042112818.jpg', 'link': 'https://financialmodelingprep.com/market-news/fmp-apogee-shares-surge-8-after-q1-beat-despite-tariff-headwinds', 'author': 'Davit Kirakosyan', 'source': 'Financial Modeling Prep'}, {'title': 'Guggenheim Boosts Disney Price Target to $140, Sees Stronger Profit and DTC Momentum', 'date': '2025-06-27 16:32:00', 'content': 'Guggenheim raised its price target on Walt Disney (NYSE:DIS) to $140 from $120 while maintaining a Buy rating, citing improved operating forecasts, resilient theme park trends, and a clearer path to direct-to-consumer (DTC) growth.\\nThe firm updated its model to reflect several positives, including lower operating expenses at Linear Networks thanks to the Star India divestiture and ongoing cost efficiencies. While recent films like Elio and Thunderbolts underperformed modestly at the box office, Sports advertising revenue is tracking better than expected, buoyed by overall audience growth during the NBA Finals.\\nMeanwhile, Disney’s Experiences division continues to show resilient attendance and travel demand, supporting earnings stability.\\nGuggenheim highlighted Disney’s upcoming full ownership of Hulu—secured with a $439 million payment to Comcast due by July 24—as a catalyst for advancing its unified DTC strategy. This includes integrating Hulu with Disney+ and the upcoming ESPN streaming service, creating new bundling opportunities to boost revenue.\\nThe analysts now forecast fiscal Q3 segment operating income of $4.5 billion, up from $4.4 billion previously, lifting full-year segment operating income to $17.7 billion—slightly ahead of the $17.65 billion Street consensus. The updated outlook reinforces Disney’s potential for sustainable profit growth and stronger competitive positioning in streaming.', 'tickers': 'NYSE:DIS', 'image': 'https://cdn.financialmodelingprep.com/images/fmp-1751041954291.jpg', 'link': 'https://financialmodelingprep.com/market-news/fmp-guggenheim-boosts-disney-price-target-to-$140-sees-stronger-profit-and-dtc-momentum', 'author': 'Davit Kirakosyan', 'source': 'Financial Modeling Prep'}, {'title': 'Goldman Raises Robinhood Target, Stays Bullish Despite Mixed Trading Volumes', 'date': '2025-06-27 16:29:00', 'content': 'Goldman Sachs lifted its price target on Robinhood Markets (NASDAQ:HOOD) to $91 from $82 while maintaining a Buy rating, citing continued product innovation and long-term growth prospects despite mixed June trading metrics.\\nRobinhood’s latest monthly data showed equity and options volumes outperforming consensus estimates, though trailing Goldman’s own forecasts, while crypto volumes fell short of both. Specifically, estimated June volumes for equities, options, and crypto were down 2%, 9%, and 30% versus Goldman’s estimates, but still came in well above the Street’s implied numbers for equities (+52%) and options (+12%), while crypto was 16% below consensus.\\nDespite these near-term volume headwinds—reflecting softer retail trading activity in June—Goldman remains optimistic about Robinhood’s structural growth trajectory. The company continues to roll out new products and expand internationally, setting the stage for broader market share gains.\\nGoldman modestly reduced its 2025 and 2026 EPS estimates by 7% and 3% respectively but kept them slightly above consensus, reflecting confidence in Robinhood’s ability to capitalize on longer-term trends in retail investing. The analysts believe the ongoing innovation and geographic expansion should underpin sustainable revenue and earnings growth, justifying the higher price target.', 'tickers': 'NASDAQ:HOOD', 'image': 'https://cdn.financialmodelingprep.com/images/fmp-1751041782750.jpg', 'link': 'https://financialmodelingprep.com/market-news/fmp-goldman-raises-robinhood-target-stays-bullish-despite-mixed-trading-volumes', 'author': 'Davit Kirakosyan', 'source': 'Financial Modeling Prep'}, {'title': 'BTIG Hikes AeroVironment Price Target, Sees Strong Demand Justifying Recent Surge', 'date': '2025-06-27 16:24:00', 'content': 'BTIG raised its price target on AeroVironment (NASDAQ:AVAV) to $300 from $225 while maintaining a Buy rating, arguing that the drone and defense tech specialist’s valuation remains attractive despite its recent rally.\\nShares of AeroVironment have jumped 43% week-to-date, but BTIG believes the stock is still compelling given robust demand in the current defense spending environment. The analyst highlighted that, based on estimated enterprise value, AeroVironment trades at roughly 8x 2025 pro forma sales—well below multiples seen among private defense tech peers, which often command double-digit sales multiples.\\nBTIG contends that AeroVironment’s growth potential, driven by rising global demand for unmanned systems and cutting-edge defense technologies, supports further upside, and the stock’s valuation is still reasonable relative to its long-term opportunities.', 'tickers': 'NASDAQ:AVAV', 'image': 'https://cdn.financialmodelingprep.com/images/fmp-1751041492400.jpg', 'link': 'https://financialmodelingprep.com/market-news/fmp-btig-hikes-aerovironment-price-target-sees-strong-demand-justifying-recent-surge', 'author': 'Davit Kirakosyan', 'source': 'Financial Modeling Prep'}, {'title': 'Progress Software Corporation (NASDAQ:PRGS) Stock Analysis and Price Target Update', 'date': '2025-06-27 13:03:07', 'content': \"John Difucci from Guggenheim sets a price target of $83 for NASDAQ:PRGS, indicating a potential increase of about 30.2%.\\nProgress Software is expected to report a significant increase in earnings per share and quarterly revenue, reflecting strong financial performance and market expansion.\\nDespite some volatility, analyst ratings and market capitalization suggest a cautiously optimistic outlook for PRGS's future performance.\\nProgress Software Corporation (NASDAQ:PRGS) is a company that provides software solutions for businesses, helping them develop and deploy applications efficiently. The company is known for its robust software products and services that cater to a wide range of industries. As of June 27, 2025, John Difucci from Guggenheim set a price target of $83 for PRGS, suggesting a potential increase of about 30.2% from its current price of $63.75.\\nProgress Software is preparing to release its second-quarter earnings results on June 30. Analysts expect the company to report earnings of $1.30 per share, up from $1.09 per share in the same period last year. This anticipated growth in earnings per share indicates strong financial performance and could contribute to the stock reaching the $83 target set by Guggenheim.\\nThe company is also expected to report quarterly revenue of $237 million, a significant increase from $175 million a year earlier. This growth in revenue reflects the company's ability to expand its market presence and deliver value to its customers. In the previous quarter, Progress Software exceeded financial expectations, which may further bolster investor confidence in the stock.\\nRecently, shares of Progress Software rose by 0.3%, closing at $63.75. DA Davidson analyst Lucky Schreiner has maintained a Buy rating on the stock, although the price target has been adjusted from $75 to $70. This adjustment indicates a cautious yet optimistic outlook on the stock's future performance.\\nThe stock for PRGS has shown some volatility, with a low of $63.45 and a high of $64.56 today. Over the past year, it has reached a high of $70.56 and a low of $50.68. With a market capitalization of approximately $2.74 billion and a trading volume of 408,650 shares, PRGS remains a significant player in the software industry.\", 'tickers': 'NASDAQ:PRGS', 'image': 'https://portal.financialmodelingprep.com/positions/685ee54e382870c8df57a559.png', 'link': 'https://financialmodelingprep.com/market-news/progress-software-corporation-nasdaq-prgs-stock-analysis-price-target-update', 'author': 'Andrew Wynn', 'source': 'Financial Modeling Prep'}, {'title': 'MSC Industrial Direct Co., Inc. (NYSE: MSC) Stock Update and Earnings Forecast', 'date': '2025-06-27 13:00:16', 'content': 'Loop Capital Markets maintains a \"Hold\" rating for MSC Industrial Direct Co., Inc. (NYSE: MSC) with a current stock price of $3.37.\\nAnalysts predict a decrease in earnings per share to $1.03 from $1.33 in the same quarter last year, indicating challenges in the industrial supply sector.\\nMSC Industrial Direct Co., Inc. (NYSE: MSC) is a leading distributor of metalworking and maintenance, repair, and operations (MRO) products and services. The company serves a wide range of industries, including manufacturing, government, and healthcare. MSC competes with other industrial supply companies like Grainger and Fastenal. On June 27, 2025, Loop Capital Markets maintained its \"Hold\" rating for MSC, with the stock priced at $3.37 at the time.\\nAs MSC prepares to release its third-quarter earnings on July 1, analysts expect earnings of $1.03 per share, down from $1.33 per share in the same period last year. This anticipated decline in earnings reflects ongoing challenges in the industrial supply sector. Despite these challenges, MSC\\'s stock has shown resilience, with a recent 3.5% increase, closing at $84.77 on Thursday.\\nThe company is expected to report quarterly revenue of $969 million, slightly down from $979 million a year earlier. This follows a 4.7% year-over-year decline in second-quarter net sales, which totaled $891.7 million, falling short of the consensus estimate of $899.54 million. Despite these revenue challenges, MSC\\'s stock has experienced a notable increase of approximately 9.78%, with a change of $0.30.\\nMSC\\'s stock has seen significant fluctuations over the past year, with a low of $2.30 and a high of $9.30. The stock\\'s current price of $3.37 reflects its recent upward trend, reaching a high of $3.37 today. The company\\'s market capitalization stands at approximately $162.26 million, with a trading volume of 8,412 shares.', 'tickers': 'NYSE:MSC', 'image': 'https://portal.financialmodelingprep.com/positions/685ea8e8382870c8df56c329.jpeg', 'link': 'https://financialmodelingprep.com/market-news/msc-industrial-direct-nyse-msc-stock-earnings-forecast', 'author': 'Gordon Thompson', 'source': 'Financial Modeling Prep'}]\n"
     ]
    }
   ],
   "source": [
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(articles)[[\"content\"]]\n",
    "df.to_csv(\"fmp_1000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d9f2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Arteris, Inc. (NASDAQ:AIP) received the \"AI Engineering Innovation Award\" for its FlexGen technology, enhancing system-on-chip (SoC) development for AI applications.\\nFlexGen technology significantly improves NoC design efficiency, with early adopters reporting a tenfold productivity increase and notable reductions in wire length and latency.\\nDespite its technological advancements, Arteris faces financial challenges, including a negative P/E ratio of -12.18 and a high enterprise value to operating cash flow ratio of 236.35, indicating potential overvaluation.\\nArteris, Inc. (Nasdaq: AIP) is a key player in the semiconductor industry, specializing in system IP for system-on-chip (SoC) development. The company recently received the \"AI Engineering Innovation Award\" at the 2025 AI Breakthrough Awards for its FlexGen technology. This innovation addresses the challenges of network-on-chip (NoC) design, particularly for AI-centric semiconductors used in data centers and self-driving cars.\\nFlexGen automates NoC creation, enhancing performance and efficiency for AI-driven SoCs. Early adopters report a tenfold productivity increase, a 30% reduction in wire length, and a 10% latency reduction. This technology also offers manual editing through FlexNoC 5, complementing its automated features. Despite these advancements, AIP\\'s financial metrics reveal challenges, such as a negative P/E ratio of -12.18, indicating current unprofitability.\\nThe AI Breakthrough Awards recognize innovation in AI, with over 5,000 nominations in 2025. Arteris\\' FlexGen technology plays a crucial role in advancing AI silicon designs. However, financial figures show a price-to-sales ratio of 6.60 and an enterprise value to sales ratio of 6.41, suggesting investors are paying a premium for each dollar of sales, despite the company\\'s negative earnings.\\nArteris\\' financial health is mixed. The company has a debt-to-equity ratio of -0.94, indicating more equity than debt, and a current ratio of 1.02, suggesting reasonable short-term financial health. However, the enterprise value to operating cash flow ratio is high at 236.35, pointing to potential overvaluation based on cash flow. Despite these challenges, Arteris remains committed to innovation in the AI landscape.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df[\"content\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee0bac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to merged_bbc_fmp.csv\n",
      "Original CSV rows: 2127\n",
      "JSONL rows added: 2000\n",
      "Total merged rows: 4127\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = 'bbc.csv'\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the JSONL file and convert to DataFrame\n",
    "jsonl_path = 'fmp_full_sums.jsonl'\n",
    "data = []\n",
    "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "df_jsonl = pd.DataFrame(data)\n",
    "\n",
    "# Make sure both DataFrames have the same column names\n",
    "# If your CSV has different column names, adjust as needed\n",
    "df_jsonl = df_jsonl.rename(columns={\n",
    "    'text': 'text',\n",
    "    'summary': 'summary'\n",
    "})\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "merged_df = pd.concat([df_csv, df_jsonl], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame back to CSV\n",
    "merged_df.to_csv('merged_bbc_fmp3.csv', index=False)\n",
    "\n",
    "print(f\"Merged data saved to merged_bbc_fmp.csv\")\n",
    "print(f\"Original CSV rows: {len(df_csv)}\")\n",
    "print(f\"JSONL rows added: {len(df_jsonl)}\")\n",
    "print(f\"Total merged rows: {len(merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d9860b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at human-centered-summarization/financial-summarization-pegasus and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1594\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1593\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1737\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1733\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1631\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1633\u001b[39m         [\n\u001b[32m   1634\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1635\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1636\u001b[39m         ]\n\u001b[32m   1637\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1596\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1598\u001b[39m     )\n\u001b[32m   1600\u001b[39m bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[31mValueError\u001b[39m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummarization\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman-centered-summarization/financial-summarization-pegasus\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1137\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1134\u001b[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001b[32m   1135\u001b[39m             tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m         tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[32m   1143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1068\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1065\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1070\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2014\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2011\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2012\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2014\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2017\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2022\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2023\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2260\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2258\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2260\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2262\u001b[39m     logger.info(\n\u001b[32m   2263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2264\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2265\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\models\\pegasus\\tokenization_pegasus_fast.py:136\u001b[39m, in \u001b[36mPegasusTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m from_slow = from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pad_token) != \u001b[33m\"\u001b[39m\u001b[33m<pad>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(eos_token) != \u001b[33m\"\u001b[39m\u001b[33m</s>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(unk_token) != \u001b[33m\"\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33madded_tokens_decoder\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_token_sent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token_sent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_slow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_slow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programming\\Python\\finance_rag\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1739\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1735\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1736\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1737\u001b[39m     ).converted()\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1743\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"human-centered-summarization/financial-summarization-pegasus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb385b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "# Load your CSV file\n",
    "input_csv = \"/content/fmp_1000.csv\"\n",
    "output_jsonl = \"summaries.jsonl\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Process each row and save summaries\n",
    "with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['content']  # Assuming your column is named 'content'\n",
    "\n",
    "        # Generate summary (adjust max_length/min_length as needed)\n",
    "        summary = summary = pipe(\n",
    "            text,  # Don't manually truncate - let the tokenizer handle it\n",
    "            max_length=100,  # Reduced from 150\n",
    "            min_length=30,\n",
    "            do_sample=False,\n",
    "            truncation=True  # Let tokenizer handle truncation properly\n",
    "        )[0]['summary_text']\n",
    "\n",
    "        # Create JSON object\n",
    "        result = {\n",
    "            \"text\": text,\n",
    "            \"summary\": summary\n",
    "        }\n",
    "\n",
    "        # Write as JSON line\n",
    "        f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        # Print progress every 10 items\n",
    "        if index % 10 == 0:\n",
    "            print(f\"Processed {index + 1}/{len(df)} items\")\n",
    "\n",
    "print(f\"Summarization complete! Results saved to {output_jsonl}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
